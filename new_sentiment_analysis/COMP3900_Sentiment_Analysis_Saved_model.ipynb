{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COMP3900_Sentiment _Analysis_Saved_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0BigxzwU0KC7",
        "dJ-QDdHE8kvs"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BigxzwU0KC7"
      },
      "source": [
        "## Unzip all saved model and load necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWhq1VyEMSM7",
        "outputId": "5d819147-9d5a-4f05-f265-be1a281b13af"
      },
      "source": [
        "!unzip saved_model.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  saved_model.zip\n",
            "replace saved_model/count_vector.pickel? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: saved_model/count_vector.pickel  \n",
            "  inflating: saved_model/nn/variables/variables.data-00000-of-00001  \n",
            "  inflating: saved_model/nn/variables/variables.index  \n",
            "  inflating: saved_model/nn/saved_model.pb  \n",
            "  inflating: saved_model/svc.pkl     \n",
            "  inflating: saved_model/lstm_encoder.pickel  \n",
            "  inflating: saved_model/lstm/variables/variables.data-00000-of-00001  \n",
            "  inflating: saved_model/lstm/variables/variables.index  \n",
            "  inflating: saved_model/lstm/saved_model.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls5AlItqL0Z0",
        "outputId": "6290cea4-98ca-4c75-fea7-1b49e7e972f2"
      },
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install tensorflow-datasets\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (20.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.29.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.12.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (3.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets) (54.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsKNxgKAL0Z7",
        "outputId": "a8a333f7-2642-4499-d34d-2f99d49550a9"
      },
      "source": [
        "import pickle\n",
        "import joblib\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.svm import SVC\n",
        "import tensorflow as tf\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ-QDdHE8kvs"
      },
      "source": [
        "# 1. Benchmark - Using VadarSentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB1tO5R5HxJq"
      },
      "source": [
        "# New words and values to update the Lexicon.\n",
        "new_words = {\n",
        "    'crushes': 10,\n",
        "    'beats': 5,\n",
        "    'increase':10,\n",
        "    'increasing':10,\n",
        "    'long': 50,\n",
        "    'misses': -5,\n",
        "    'trouble': -10,\n",
        "    'falls': -100,\n",
        "    'drops':-100,\n",
        "    'dropping':-200,\n",
        "    'falling':-100,\n",
        "    \n",
        "}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_VEmU0e8YpA",
        "outputId": "91816e07-6d03-411e-8c7d-58223b300c41"
      },
      "source": [
        "analyser = SentimentIntensityAnalyzer()\n",
        "# Example\n",
        "score = analyser.polarity_scores(\"Apple Stock Is Falling Again. Why That’s Not a Problem for the Dow.\")\n",
        "print(score)\n",
        "analyser.lexicon.update(new_words)\n",
        "# Example\n",
        "score = analyser.polarity_scores(\"Apple Stock Is Falling Again. Why That’s Not a Problem for the Dow.\")\n",
        "print(score)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'neg': 0.108, 'neu': 0.74, 'pos': 0.152, 'compound': 0.1675}\n",
            "{'neg': 0.884, 'neu': 0.096, 'pos': 0.02, 'compound': -0.9992}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSqKRig48ony"
      },
      "source": [
        "# 2. Using LSTM model trained on IMDB dataset from tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB3EfIHI0X4J"
      },
      "source": [
        "### Preprocessing functions for LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2exMcghb8edT"
      },
      "source": [
        "def pad_to_size(vec, size):\n",
        "    zeros = [0] * (size - len(vec))\n",
        "    vec.extend(zeros)\n",
        "    return vec\n",
        "\n",
        "def sample_predict(sample_pred_text, encoder, pad, model):\n",
        "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "    if pad:\n",
        "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
        "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "\n",
        "    return (predictions)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SZ63yLI_8Zs"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKIk7fFkL0Z-",
        "outputId": "83f7b9bf-b5f8-48a1-9122-7e31355a2f4e"
      },
      "source": [
        "encoder_loaded = pickle.load(open(\"saved_model/lstm_encoder.pickel\", \"rb\"))\n",
        "lstm_loaded = tf.keras.models.load_model('saved_model/lstm')\n",
        "\n",
        "# Check its architecture\n",
        "lstm_loaded.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 64)          523840    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 128)         66048     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 635,329\n",
            "Trainable params: 635,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug_Nhg7d0dG3"
      },
      "source": [
        "### Prediction Example by LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo_5jdLrCEbY",
        "outputId": "ec4998d8-373c-491b-9fa0-703172caa01e"
      },
      "source": [
        "new_prediction = sample_predict(\"Apple Stock Is Falling Again. Why That’s Not a Problem for the Dow.\", encoder = encoder_loaded, pad=False, model = lstm_loaded)\n",
        "print(new_prediction)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.48384848]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAhNTcRm-yGM"
      },
      "source": [
        "# 3. Building a Neural Network and train on IMDB Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlWR0U_GlaiV"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bZUtKGwwPUy"
      },
      "source": [
        "TOP_WORDS = 10000\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOlqRujwmGQC"
      },
      "source": [
        "\n",
        "word2index = imdb.get_word_index()\n",
        "word2index = {k:(v+3) for k,v in word2index.items()}\n",
        "\n",
        "def clean_symbols(text):\n",
        "    \n",
        "    for char in text:\n",
        "        # remove punctuation but preserve symbols defined above \n",
        "        if char in string.punctuation and char != ' ':\n",
        "            text = text.replace(char, '')\n",
        "        # remove all other characters\n",
        "        if char.isalpha() is False and char.isdigit() is False and char != ' ':\n",
        "            text = text.replace(char, '')\n",
        "          \n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def clean_words(news, dimension = TOP_WORDS):\n",
        "    cleaned = clean_symbols(news).lower()\n",
        "    test=[]\n",
        "    for word in word_tokenize(cleaned):\n",
        "        if word in word2index:\n",
        "            test.append(word2index[word])\n",
        "    \n",
        "    results = np.zeros(dimension)\n",
        "    for _ , sequence in enumerate(test):\n",
        "        if sequence < dimension:\n",
        "            results[sequence] = 1\n",
        "    \n",
        "    results = np.reshape(results,(1, TOP_WORDS))\n",
        "\n",
        "    return results"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf41-ZB0Cgqc"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JZB9LESCgBG",
        "outputId": "fc22ed5a-3c47-4c1a-9604-ce794b99db4d"
      },
      "source": [
        "NN_loaded = tf.keras.models.load_model('saved_model/nn')\n",
        "\n",
        "# Check its architecture\n",
        "NN_loaded.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                816       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 505,983\n",
            "Trainable params: 505,983\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUHUUOys1IyH",
        "outputId": "dc386a42-abcb-45c5-9b14-28a3b2e8e99f"
      },
      "source": [
        "def predict_sentiment(x_test, NN):\n",
        "\n",
        "    x_test = clean_words(x_test, dimension = TOP_WORDS)\n",
        "    #print(x_test.shape)\n",
        "    prediction = NN.predict(x_test) \n",
        "    #print(prediction)\n",
        "    return prediction\n",
        "\n",
        "predict_sentiment(\"it is good and let's have a try\", NN_loaded)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9595045]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q_XPdv2g4Sk"
      },
      "source": [
        "# SVC trained on another dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQqxfFdchuzj",
        "outputId": "5d43e454-9fb9-4df5-bc34-340c581112f6"
      },
      "source": [
        "# making list stopwords for removing stopwords from our text \n",
        "stop = set(stopwords.words('english'))\n",
        "stop.update(punctuation)\n",
        "print(stop)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'here', \"you'll\", 'who', '.', 'our', 'be', 'herself', 'below', 'through', 'more', \"she's\", 'being', 'their', 'having', \"'\", '}', 'then', ';', 'of', \"you're\", 'up', 'why', '$', 'as', \"that'll\", 't', 'didn', \"weren't\", 'your', 'haven', 'if', 'yourselves', 'have', '<', 'down', \"needn't\", '-', 'with', \"hasn't\", 'won', 'there', 're', 'hers', 'what', \"you'd\", 'when', '&', '?', '_', 'his', 'from', 'mustn', \"shan't\", 'doing', 'isn', '`', 'should', 'shouldn', 'me', 'is', ':', 'these', 'but', 'than', \"don't\", 'him', 'm', '#', 'each', ']', 'too', 'off', '\\\\', 'wasn', '%', 'against', 'ain', 'hasn', \"wouldn't\", 'both', 'not', 'so', 'over', 'can', '[', \"it's\", \"should've\", \"aren't\", 'whom', 'he', 'its', 'has', 'above', '>', '(', 'shan', 'while', 'yourself', 'during', \"didn't\", '^', \"isn't\", 'just', 'about', 'ma', 'doesn', \"mustn't\", 'does', 'ourselves', 'yours', 'further', 'after', 've', '{', 'before', 'mightn', '!', 'a', \"wasn't\", 'once', 'how', ')', \"doesn't\", '~', 'her', 'did', 'at', 'will', 'on', 'few', 'or', \"shouldn't\", 'had', 'theirs', 'been', ',', 'some', 'himself', 'were', 'that', 'out', 'now', 'll', \"mightn't\", 'those', 'myself', 'needn', '+', 'they', 'between', '=', 'no', '|', 'most', 'because', 'it', 'ours', 'for', 'an', '\"', 'the', 'this', 'itself', 'by', 'where', 'to', 'don', '*', 'other', \"you've\", 'any', \"hadn't\", 'aren', 'do', 'nor', 'into', 'y', \"haven't\", \"won't\", 'weren', 'all', 'very', 'themselves', 'was', '@', 'd', 'such', 'couldn', 'and', 'own', 's', 'am', 'i', 'only', 'under', 'hadn', 'my', 'we', 'until', 'in', 'again', 'them', 'are', 'o', 'wouldn', 'same', 'which', '/', 'you', \"couldn't\", 'she'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNd-zdTah8cT"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "def cleanText(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "    text = text.lower()\n",
        "    text = text.replace('x', '')\n",
        "\n",
        "    clean_text = []\n",
        "    for w in word_tokenize(text):\n",
        "        if w.lower() not in stop:\n",
        "            clean_text.append(w)\n",
        "    return clean_text\n",
        "    \n",
        "def join_text(text):\n",
        "    return \" \".join(text)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au0J1VTBAZms"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNNOZAelAcjP"
      },
      "source": [
        "# load pickle\n",
        "count_vec_loaded = pickle.load(open(\"saved_model/count_vector.pickel\", \"rb\"))\n",
        "\n",
        "# load\n",
        "svc_loaded = joblib.load(\"saved_model/svc.pkl\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QycqlR6RL0aE"
      },
      "source": [
        "def svc_predict(svc, count_vec, news):\n",
        "    news_processed = cleanText(news)\n",
        "    news_processed = join_text(news_processed)\n",
        "    \n",
        "    test_news = count_vec.transform([news_processed]).todense()\n",
        "    return svc.predict_proba(test_news)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA2uFLgSL0aG",
        "outputId": "67bb3077-571c-46c7-b6b2-8fc8f56e013f"
      },
      "source": [
        "print(svc_predict(svc_loaded, count_vec_loaded, \"Investors are looking to buy more stocks\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.06255801 0.72184132 0.21560067]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZbzY31ubvfU"
      },
      "source": [
        "# Combine all models together to give the final rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4rpCI14by6r"
      },
      "source": [
        "def predict_rating(news, lstm, NN):\n",
        "    benchmark_rating = analyser.polarity_scores(news)['compound']\n",
        "    print(f'result from vadar: {benchmark_rating:.2f}')\n",
        "\n",
        "    # Predicted by LSTM model, using padding = True\n",
        "    rating_lstm = sample_predict(news, encoder_loaded, pad=True, model = lstm)[0][0]\n",
        "    print(f'result from lstm: {rating_lstm:.2f}')\n",
        "  \n",
        "    # predicted by NN\n",
        "    rating_NN = predict_sentiment(news, NN)[0][0]\n",
        "    print(f'result from Neural Network: {rating_NN:.2f}')\n",
        "    \n",
        "    # predictd by SVC\n",
        "    rating_svc = svc_predict(svc_loaded, count_vec_loaded, news)[0]\n",
        "    print(f\"result from SVC: positive:{rating_svc[2]:.2f}, neutral:{rating_svc[1]:.2f}, negative:{rating_svc[0]:.2f}\")\n",
        "\n",
        "    if np.argmax(rating_svc) == 2:\n",
        "        # the news is in the positive side\n",
        "        if benchmark_rating >= 0.05 and rating_lstm >=0 and rating_NN >= 0.5:\n",
        "            return \"Strongly Positive\", rating_svc[2]\n",
        "        elif benchmark_rating >= 0.05 or rating_lstm >=0 or rating_NN >= 0.5:\n",
        "            return \"Slightly Positive\", rating_svc[2]\n",
        "        else:\n",
        "            return \"Positive\", rating_svc[2]\n",
        "\n",
        "    elif np.argmax(rating_svc) == 0:\n",
        "        # the news is in the negative side\n",
        "        if benchmark_rating <-0.05 and rating_lstm <0 and rating_NN < 0.5:\n",
        "            return \"Strongly Negative\", rating_svc[0]* -1\n",
        "        elif benchmark_rating <-0.05 or rating_lstm <0 or rating_NN < 0.5:\n",
        "            return \"Slightly Negative\", rating_svc[0] * -1\n",
        "        else:\n",
        "            return \"Negative\", rating_svc[0] * -1\n",
        "\n",
        "    else:\n",
        "        if benchmark_rating < -0.05:\n",
        "            return \"Negative\", rating_svc[0] * -1\n",
        "        if benchmark_rating > 0.05:\n",
        "            return \"Positive\", rating_svc[2]\n",
        "        if rating_svc[1] > 0.7:\n",
        "            return \"Neutral\", rating_svc[1] * 0\n",
        "        else:\n",
        "            if rating_svc[0] > rating_svc[2]:\n",
        "                return \"Slightly Negative\", rating_svc[0] * -1\n",
        "            else:\n",
        "                return \"Slightly Positive\", rating_svc[2]\n",
        "\n",
        "            \n",
        "    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YqoxWmc1p2z"
      },
      "source": [
        "# Prediction Example by using the final combined model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LntuJBtieSBx",
        "outputId": "99745bad-e1c9-4e23-ff76-07f407a327a6"
      },
      "source": [
        "predict_rating(\"Apple Stock Is Falling Again. Why That’s Not a Problem for the Dow.\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: -1.00\n",
            "result from lstm: -0.31\n",
            "result from Neural Network: 0.00\n",
            "result from SVC: positive:0.09, neutral:0.89, negative:0.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Negative', -0.02164393735425806)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw_M-HFQeWYV",
        "outputId": "3097f372-09d4-432e-ab4e-b4a76e9cecd7"
      },
      "source": [
        "predict_rating(\"Apple is launching new products in the next month and it displays good prospect. Investors are keen to invest\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: 0.77\n",
            "result from lstm: 2.15\n",
            "result from Neural Network: 1.00\n",
            "result from SVC: positive:0.32, neutral:0.65, negative:0.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Positive', 0.3216184616392452)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5bOgmOtkqgB",
        "outputId": "fa691220-1ed0-42c1-8013-0a2e1c3fc04d"
      },
      "source": [
        "predict_rating(\"'Total blackout': Facebook, Instagram crashes across the globe\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: 0.00\n",
            "result from lstm: 3.63\n",
            "result from Neural Network: 0.90\n",
            "result from SVC: positive:0.02, neutral:0.94, negative:0.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Neutral', 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Swd2jvaoTLl",
        "outputId": "fee5b2a9-ea50-442a-a945-c278188db1ae"
      },
      "source": [
        "predict_rating(\"The market is falling\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: -1.00\n",
            "result from lstm: 0.48\n",
            "result from Neural Network: 0.22\n",
            "result from SVC: positive:0.05, neutral:0.93, negative:0.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Negative', -0.018970463804799848)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgCSqhJsTJMG",
        "outputId": "748277ee-5b1c-4d4a-c835-282f558f6375"
      },
      "source": [
        "predict_rating(\"Vodafone suffers network outage\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: -0.48\n",
            "result from lstm: 0.48\n",
            "result from Neural Network: 0.11\n",
            "result from SVC: positive:0.05, neutral:0.93, negative:0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Negative', -0.014486554855328676)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEYNgEpZXGRm",
        "outputId": "c6259c52-1116-4216-86e5-bb5e0fe1c5ce"
      },
      "source": [
        "predict_rating(\"Facebook is generating high profit in the current quarter\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: 0.44\n",
            "result from lstm: 0.57\n",
            "result from Neural Network: 0.01\n",
            "result from SVC: positive:0.46, neutral:0.30, negative:0.23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Slightly Positive', 0.4630470420612455)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKq9tnytX0Gg",
        "outputId": "21e72d36-f380-4e32-93de-7da4b34c04f1"
      },
      "source": [
        "predict_rating(\"Tesla is generating large operating loss in the current quarter\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: -0.32\n",
            "result from lstm: -2.43\n",
            "result from Neural Network: 0.00\n",
            "result from SVC: positive:0.07, neutral:0.07, negative:0.86\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Strongly Negative', -0.8609715066051844)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riZ3Ip8A9l3",
        "outputId": "f5faa230-80f3-4046-c720-208e04bafafc"
      },
      "source": [
        "predict_rating(\"SoftBank is under investigation by the SEC following its risky 'Nasdaq whale' investments (UBER, AAPL, TSLA)\", lstm_loaded, NN_loaded)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: -0.20\n",
            "result from lstm: -2.01\n",
            "result from Neural Network: 0.53\n",
            "result from SVC: positive:0.09, neutral:0.90, negative:0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Negative', -0.011860595932658261)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m49K_AyoBCCM",
        "outputId": "52c7cc98-8358-4ba8-ca9f-65463a83735f"
      },
      "source": [
        "predict_rating(\"Apple can definitely build a car — but it wouldn't want to sell it in the US (AAPL)\", lstm_loaded, NN_loaded)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result from vadar: 0.13\n",
            "result from lstm: -0.03\n",
            "result from Neural Network: 0.99\n",
            "result from SVC: positive:0.06, neutral:0.89, negative:0.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Positive', 0.06271062598495668)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}